{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KSohi-max/Sherlock_Holmes_NLP/blob/main/Final_Term_Project_3666_03_ANLP_ChatBot_Sherlock_Holmes_Trivia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlR-C_5fyeou"
      },
      "source": [
        "#Setup Environment (Libraries)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ani3XTRX_an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca08ba08-1c86-4e7a-c116-028927d42dda"
      },
      "source": [
        "!pip install nltk==3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.62.3)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434694 sha256=df0f316cda35fa9c6bda33660def597bd16f757ec17ed2600e0a09ff0e3fa231\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV3NVsxAJEAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f84738b-b491-473b-8b3b-c0938a00fb27"
      },
      "source": [
        "import platform\n",
        "cloud_computing = (platform.system()==\"Linux\") #if on Linux, it must be on the cloud\n",
        "print(\"Current System:\", platform.system(), \"(\"+platform.machine()+\")\", \"\\nCloud Computing:\", cloud_computing)\n",
        "import psutil\n",
        "print(\"CPU Count: \", psutil.cpu_count(logical=False), \"/\", psutil.cpu_count(), \"(\"+platform.processor()+\")\")\n",
        "print(\"Total Memory: \", psutil.virtual_memory().total >> 30, \"GB\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current System: Linux (x86_64) \n",
            "Cloud Computing: True\n",
            "CPU Count:  2 / 4 (x86_64)\n",
            "Total Memory:  25 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSiQkkrNnbcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adb420c-ced8-42c2-cf80-1e43b121ca22"
      },
      "source": [
        "%%time\n",
        "from IPython.display import display\n",
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22 s, sys: 8.06 s, total: 30.1 s\n",
            "Wall time: 1min 58s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "T4bTyGtXG9Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHrGZZP8G_OK",
        "outputId": "bc9c0e32-88ea-4bcb-9c58-7ae3149da74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/drive/My Drive/pickled_Sherlock_Holmes_corpus.zip' '/content'"
      ],
      "metadata": {
        "id": "nE6epzbrIX-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#source_dir='/content/drive/MyDrive/3666 - Applied Natural Language Processing/Term Project'\n",
        "target_dir='/content'\n",
        "!rm -rf $target_dir/pickled_Sherlock_Holmes_corpus/*\n",
        "!cp -p \"$source_dir\"/pickled_Sherlock_Holmes_corpus.zip $target_dir\n",
        "!unzip $target_dir/pickled_Sherlock_Holmes_corpus\n",
        "# (Optional) Force rebuilding the Ball Tree model\n",
        "!rm -rf $target_dir/*.pkl\n",
        "# (Optional) Copy the picked Ball Tree model if the previously saved one is in good shape\n",
        "# This saves time to rebuild the model from scratch\n",
        "!cp -p \"$source_dir\"/*.pkl $target_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDDpac-QJmDh",
        "outputId": "5d3f043f-70ea-4176-c85b-ce007fd5eaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: missing destination file operand after '/pickled_Sherlock_Holmes_corpus.zip'\n",
            "Try 'cp --help' for more information.\n",
            "Archive:  /content/pickled_Sherlock_Holmes_corpus.zip\n",
            "  inflating: pickled_Sherlock_Holmes_corpus/A_Case_of_Identity.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/A_Scandal_in_Bohemia.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/A_Study_In_Scarlet.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Black_Peter.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Charles_Augustus_Milverton.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/His_Last_Bow.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Lady_Frances_Carfax.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Shoscombe_Old_Place.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Silver_Blaze.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Abbey_Grange.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Adventure_of_the_Beryl_Coronet.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Adventure_of_the_Blue_Carbuncle.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Adventure_of_the_Copper_Beeches.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Adventure_of_the_Engineer_s_Thumb.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Adventure_of_the_Noble_Bachelor.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Adventure_of_the_Speckled_Band.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Blanched_Soldier.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Boscombe_Valley_Mystery.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Bruce-Partington_Plans.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Cardboard_Box.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Creeping_Man.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Crooked_Man.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Dancing_Men.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Devil_s_Foot.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Dying_Detective.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Empty_House.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Final_Problem.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Five_Orange_Pips.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Gloria_Scott.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Golden_Pince-Nez.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Greek_Interpreter.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Hound_of_the_Baskervilles.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Illustrious_Client.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Lion_s_Mane.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Man_with_the_Twisted_Lip.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Mazarin_Stone.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Missing_Three-Quarter.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Musgrave_Ritual.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Naval_Treaty.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Norwood_Builder.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Priory_School.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Red_Circle.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Red-Headed_League.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Reigate_Puzzle.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Resident_Patient.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Retired_Colourman.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Second_Stain.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Sign_of_the_Four.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Six_Napoleons.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Solitary_Cyclist.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Stockbroker_s_Clerk.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Sussex_Vampire.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Three_Gables.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Three_Garridebs.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Three_Students.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Valley_of_Fear.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/The_Veiled_Lodger.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Thor_Bridge.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Wisteria_Lodge.pickle  \n",
            "  inflating: pickled_Sherlock_Holmes_corpus/Yellow_Face.pickle  \n",
            "cp: missing destination file operand after '/*.pkl'\n",
            "Try 'cp --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l $target_dir/pickled_Sherlock_Holmes_corpus | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON8-nTGTJ84Q",
        "outputId": "b1c5c91e-7db9-4739-d344-30ebe6cef933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 9240\n",
            "-rw-r--r-- 1 root root 100594 Dec  4 14:38 A_Case_of_Identity.pickle\n",
            "-rw-r--r-- 1 root root 122574 Dec  4 14:38 A_Scandal_in_Bohemia.pickle\n",
            "-rw-r--r-- 1 root root 638824 Dec  4 14:38 A_Study_In_Scarlet.pickle\n",
            "-rw-r--r-- 1 root root 114077 Dec  4 14:38 Black_Peter.pickle\n",
            "-rw-r--r-- 1 root root  95704 Dec  4 14:38 Charles_Augustus_Milverton.pickle\n",
            "-rw-r--r-- 1 root root  88957 Dec  4 14:38 His_Last_Bow.pickle\n",
            "-rw-r--r-- 1 root root 109542 Dec  4 14:38 Lady_Frances_Carfax.pickle\n",
            "-rw-r--r-- 1 root root  90796 Dec  4 14:38 Shoscombe_Old_Place.pickle\n",
            "-rw-r--r-- 1 root root 134722 Dec  4 14:38 Silver_Blaze.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3Pp3c9o9TW"
      },
      "source": [
        "# Dialog"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggRW-q-HG0jH"
      },
      "source": [
        "import abc\n",
        "import re\n",
        "from collections.abc import Sequence\n",
        "from operator import itemgetter\n",
        "\n",
        "class Dialog(abc.ABC):\n",
        "    \"\"\"\n",
        "    A dialog listens for utterances, parses and interprets them, then updates\n",
        "    its internal state. It can then formulate a response on demand.\n",
        "    \"\"\"\n",
        "\n",
        "    def listen(self, text, response=True, **kwargs):\n",
        "        \"\"\"\n",
        "        A text utterance is passed in and parsed. It is then passed to the\n",
        "        interpret method to determine how to respond. If a response is\n",
        "        requested, the respond method is used to generate a text response\n",
        "        based on the most recent input and the current Dialog state.\n",
        "        \"\"\"\n",
        "        # Parse the input\n",
        "        sents = self.parse(text)\n",
        "\n",
        "        # Interpret the input\n",
        "        sents, confidence, kwargs = self.interpret(sents, **kwargs)\n",
        "\n",
        "        # Determine the response\n",
        "        if response:\n",
        "            reply = self.respond(sents, confidence, **kwargs)\n",
        "        else:\n",
        "            reply = None\n",
        "\n",
        "        # Return initiative\n",
        "        return reply, confidence\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def parse(self, text):\n",
        "        \"\"\"\n",
        "        Every dialog may need its own parsing strategy, some dialogs may need\n",
        "        dependency vs. constituency parses, others may simply require regular\n",
        "        expressions or chunkers.\n",
        "        \"\"\"\n",
        "        return []\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def interpret(self, sents, **kwargs):\n",
        "        \"\"\"\n",
        "        Interprets the utterance passed in as a list of parsed sentences,\n",
        "        updates the internal state of the dialog, computes a confidence of the\n",
        "        interpretation. May also return arguments specific to the response\n",
        "        mechanism.\n",
        "        \"\"\"\n",
        "        return sents, 0.0, kwargs\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def respond(self, sents, confidence, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a response given the input utterances and the current state of\n",
        "        the dialog, along with any arguments passed in from the listen or the\n",
        "        interpret methods.\n",
        "        \"\"\"\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpZzrAdmzjUM"
      },
      "source": [
        "# Define Text Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBhyW1akzgMA"
      },
      "source": [
        "import nltk\n",
        "import unicodedata\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "\n",
        "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, language='english', minimum=2, maximum=200):\n",
        "        self.min = minimum\n",
        "        self.max = maximum\n",
        "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def is_punct(self, token):\n",
        "        return all(\n",
        "            unicodedata.category(char).startswith('P') for char in token\n",
        "        )\n",
        "\n",
        "    def is_stopword(self, token):\n",
        "        return token.lower() in self.stopwords\n",
        "\n",
        "    def normalize(self, document):\n",
        "        return [\n",
        "            self.lemmatize(token, tag).lower()\n",
        "            for paragraph in document\n",
        "            for sentence in paragraph\n",
        "            for (token, tag) in sentence\n",
        "            if token in list(self.reduced)\n",
        "               and not self.is_punct(token) and not self.is_stopword(token)\n",
        "        ]\n",
        "\n",
        "    def lemmatize(self, token, pos_tag):\n",
        "        tag = {\n",
        "            'N': wn.NOUN,\n",
        "            'V': wn.VERB,\n",
        "            'R': wn.ADV,\n",
        "            'J': wn.ADJ\n",
        "        }.get(pos_tag[0], wn.NOUN)\n",
        "\n",
        "        return self.lemmatizer.lemmatize(token, tag)\n",
        "\n",
        "    def fit(self, documents, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        words = []\n",
        "        docs = []\n",
        "        for document in documents:\n",
        "            docs.append(document)\n",
        "            for para in document:\n",
        "                for sent in para:\n",
        "                    for token, tag in sent:\n",
        "                        words.append(token)\n",
        "\n",
        "        counts = FreqDist(words)\n",
        "        self.reduced = set(\n",
        "            w for w in words if counts[w] > self.min and counts[w] < self.max\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            ' '.join(self.normalize(doc)) for doc in docs\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwDk73ppuzJ-"
      },
      "source": [
        "#  Ball Tree Algorithm (Nearest Neighbor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isG_-lMUtujO"
      },
      "source": [
        "## Building the Nearest Neighbor model\n",
        "\n",
        "import pickle\n",
        "#from sklearn.externals import joblib\n",
        "# Above import gives error: ImportError: cannot import name 'joblib' from 'sklearn.externals' (/usr/local/lib/python3.7/dist-packages/sklearn/externals/__init__.py)\n",
        "# Changed to directly import joblib below, after consulting StackOverflow\n",
        "import joblib\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class BallTreeRecommender(object):\n",
        "    \"\"\"\n",
        "    Given input terms, provide k story recommendations\n",
        "    \"\"\"\n",
        "    def __init__(self, k=3, **kwargs):\n",
        "        self.k = k\n",
        "        self.trans_path = \"svd.pkl\"\n",
        "        self.tree_path = \"tree.pkl\"\n",
        "        self.transformer = False\n",
        "        self.tree = None\n",
        "        self.load()\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "        Load a pickled transformer and tree from disk,\n",
        "        if they exist.\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.trans_path):\n",
        "            self.transformer = joblib.load(open(self.trans_path, 'rb'))\n",
        "            self.tree = joblib.load(open(self.tree_path, 'rb'))\n",
        "        else:\n",
        "            self.transformer = False\n",
        "            self.tree = None\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"\n",
        "        It takes a long time to fit, so just do it once!\n",
        "        \"\"\"\n",
        "        joblib.dump(self.transformer, open(self.trans_path, 'wb'))\n",
        "        joblib.dump(self.tree, open(self.tree_path, 'wb'))\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        # Transformer will be False if pipeline hasn't been fit yet,\n",
        "        # Trigger fit_transform and save the transformer and lexicon.\n",
        "        # Initially from Module 08, TruncatedSVD(n_components=200) was specified.\n",
        "        # Changed n_compoenents from 200 to 2, due to ValueError: n_components must be < n_features; got 200 >= 2\n",
        "        if self.transformer == False:\n",
        "            self.transformer = Pipeline([\n",
        "                 ('norm', TextNormalizer(minimum=50, maximum=200)),\n",
        "                #('norm', TextNormalizer(minimum=2, maximum=5000)),\n",
        "                ('transform', Pipeline([\n",
        "                    ('tfidf', TfidfVectorizer()),\n",
        "                    ('svd', TruncatedSVD(n_components=200))\n",
        "                ])\n",
        "                 )\n",
        "            ])\n",
        "            self.lexicon = self.transformer.fit_transform(documents)\n",
        "            self.tree = BallTree(self.lexicon)\n",
        "            self.save()\n",
        "\n",
        "    def query(self, terms):\n",
        "        \"\"\"\n",
        "        Given input list of noun keywords,\n",
        "        return the k closest matching stories.\n",
        "        :param terms: list of strings\n",
        "        :return: list of document indices of documents\n",
        "        \"\"\"\n",
        "        vect_doc = self.transformer.named_steps['transform'].fit_transform(\n",
        "             wordpunct_tokenize(terms) # input paramter \"terms\" is already tokenized\n",
        "            #terms\n",
        "        )\n",
        "        dists, inds = self.tree.query(vect_doc, k=self.k)\n",
        "        return inds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PickledCorpusReader"
      ],
      "metadata": {
        "id": "p5PmYbG-KLKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import codecs\n",
        "import pickle\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk import sent_tokenize\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.corpus.reader.api import CorpusReader\n",
        "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
        "\n",
        "DOC_PATTERN = r'(?!\\.)[\\w\\s\\d\\-\\_]+\\.txt'\n",
        "PKL_PATTERN = r'(?!\\.)[\\w\\s\\d\\-\\_]+\\.pickle'\n",
        "CAT_PATTERN = r'([a-zA-Z_\\s]+)/.*'\n",
        "\n",
        "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
        "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the corpus reader.  Categorization arguments\n",
        "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
        "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
        "        are passed to the ``CorpusReader`` constructor.\n",
        "        \"\"\"\n",
        "        # Add the default category pattern if not passed into the class.\n",
        "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
        "            kwargs['cat_pattern'] = CAT_PATTERN\n",
        "\n",
        "        CategorizedCorpusReader.__init__(self, kwargs)\n",
        "        CorpusReader.__init__(self, root, fileids)\n",
        "\n",
        "    def resolve(self, fileids, categories):\n",
        "        \"\"\"\n",
        "        Returns a list of fileids or categories depending on what is passed\n",
        "        to each internal corpus reader function. This primarily bubbles up to\n",
        "        the high level ``docs`` method, but is implemented here similar to\n",
        "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
        "        \"\"\"\n",
        "        if fileids is not None and categories is not None:\n",
        "            raise ValueError(\"Specify fileids or categories, not both\")\n",
        "\n",
        "        if categories is not None:\n",
        "            return self.fileids(categories)\n",
        "        return fileids\n",
        "\n",
        "    def text(self, fileids=None, categories=None):\n",
        "        \"\"\"\n",
        "        Returns the document loaded from a pickled object for every file in\n",
        "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
        "        to acheive memory safe iteration.\n",
        "        \"\"\"\n",
        "        # Resolve the fileids and the categories\n",
        "        fileids = self.resolve(fileids, categories)\n",
        "\n",
        "        # Create a generator, loading one document into memory at a time.\n",
        "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
        "            with open(path, 'rb') as f:\n",
        "                yield pickle.load(f)\n",
        "\n",
        "    def docs(self, fileids=None, categories=None):\n",
        "        \"\"\"\n",
        "        Returns a generator of paragraphs where each paragraph is a list of\n",
        "        sentences, which is in turn a list of (token, tag) tuples.\n",
        "        \"\"\"\n",
        "        for doc in self.text(fileids, categories):\n",
        "            yield doc['document']\n",
        "\n",
        "    def titles(self, fileids=None, categories=None):\n",
        "        \"\"\"\n",
        "        Uses BeautifulSoup to identify titles from the\n",
        "        head tags within the HTML\n",
        "        \"\"\"\n",
        "        for doc in self.text(fileids, categories):\n",
        "            yield doc['title']\n",
        "\n",
        "    def paras(self, fileids=None, categories=None):\n",
        "        \"\"\"\n",
        "        Returns a generator of paragraphs where each paragraph is a list of\n",
        "        sentences, which is in turn a list of (token, tag) tuples.\n",
        "        \"\"\"\n",
        "        for doc in self.docs(fileids, categories):\n",
        "            for paragraph in doc:\n",
        "                yield paragraph\n",
        "\n",
        "    def sents(self, fileids=None, categories=None):\n",
        "        \"\"\"\n",
        "        Returns a generator of sentences where each sentence is a list of\n",
        "        (token, tag) tuples.\n",
        "        \"\"\"\n",
        "        for paragraph in self.paras(fileids, categories):\n",
        "            for sentence in paragraph:\n",
        "                yield sentence\n",
        "\n",
        "    def tagged(self, fileids=None, categories=None):\n",
        "        for sent in self.sents(fileids, categories):\n",
        "            for token in sent:\n",
        "                yield token\n",
        "\n",
        "    def words(self, fileids=None, categories=None):\n",
        "        \"\"\"\n",
        "        Returns a generator of (token, tag) tuples.\n",
        "        \"\"\"\n",
        "        for token in self.tagged(fileids, categories):\n",
        "            yield token[0]"
      ],
      "metadata": {
        "id": "Ioe2t0l_KXVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vcauOKv-cK"
      },
      "source": [
        "# Story Recommender "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9mUbWYmvt5E"
      },
      "source": [
        "class StoryRecommender(Dialog):\n",
        "    \"\"\" Story recommender dialog  \"\"\"\n",
        "    def __init__(self, story, recommender=BallTreeRecommender(k=3)):\n",
        "        self.story = list(story.titles())\n",
        "        self.recommender = recommender\n",
        "        # Fit the recommender model with the corpus\n",
        "        self.recommender.fit_transform(list(story.docs()))\n",
        "\n",
        "    def parse(self, text):\n",
        "        \"\"\" Extract ingredients from the text  \"\"\"\n",
        "        return pos_tag(wordpunct_tokenize(text))\n",
        "\n",
        "    def interpret(self, sents, **kwargs):\n",
        "        # If feedback detected, update the model\n",
        "        if 'feedback' in kwargs:\n",
        "            self.recommender.update(kwargs['feedback'])\n",
        "        n_nouns = sum(1 for pos , tag in sents if pos.startswith(\"N\"))\n",
        "        confidence = n_nouns/len(sents)\n",
        "        terms = [tag for pos, tag in sents if pos.startswith(\"N\")]\n",
        "        return terms, confidence, kwargs\n",
        "      \n",
        "    def respond(self, terms, confidence, **kwargs):\n",
        "        \"\"\"Returns a recommendation if the confidence is > 0.15 otherwise None.\n",
        "        \"\"\"\n",
        "        if confidence < 0.15:\n",
        "            return None\n",
        "        output = [\"Here are stories related to {}\".format(\", \".join(terms))]\n",
        "        output += [\n",
        "            \"- {}\".format(self.story[idx])\n",
        "            for idx in self.recommender.query(terms)\n",
        "        ]\n",
        "        return \"\\n\".join(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3md6GX-wVpC"
      },
      "source": [
        "# Test Recommender 👉"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  corpus = PickledCorpusReader('pickled_Sherlock_Holmes_corpus')\n",
        "  recommender = StoryRecommender(corpus)\n",
        "  question = \"Which stories contain door, child, woman, blood, money\"\n",
        "  print (recommender.listen(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2vtr-6V2I8Z",
        "outputId": "16d091aa-74d8-424b-906d-4cd6a1544ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing corpus titles\n",
        "print(len(list(corpus.titles())))\n",
        "list(corpus.titles())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-EtEPnmQm5U",
        "outputId": "ea89102f-9e1e-46fc-8d46-4a424abf5d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A CASE OF IDENTITY',\n",
              " 'A SCANDAL IN BOHEMIA',\n",
              " 'A STUDY IN SCARLET',\n",
              " 'THE ADVENTURE OF BLACK PETER',\n",
              " 'THE ADVENTURE OF CHARLES AUGUSTUS MILVERTON']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(corpus.sents())[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI8wu2q0U0J9",
        "outputId": "dffc3e9c-1082-4caa-9993-d20909a224fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('A', 'DT'), ('CASE', 'NNP'), ('OF', 'NNP'), ('IDENTITY', 'NNP')], [('Arthur', 'NNP'), ('Conan', 'NNP'), ('Doyle', 'NNP')], [('\"', 'VB'), ('My', 'PRP$'), ('dear', 'JJ'), ('fellow', 'JJ'), (',\"', 'NN'), ('said', 'VBD'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), ('as', 'IN'), ('we', 'PRP'), ('sat', 'VBD'), ('on', 'IN'), ('either', 'DT'), ('side', 'NN'), ('of', 'IN'), ('the', 'DT'), ('fire', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('lodgings', 'NNS'), ('at', 'IN'), ('Baker', 'NNP'), ('Street', 'NNP'), (',', ','), ('\"', 'NNP'), ('life', 'NN'), ('is', 'VBZ'), ('infinitely', 'RB'), ('stranger', 'JJ'), ('than', 'IN'), ('anything', 'NN'), ('which', 'WDT'), ('the', 'DT'), ('mind', 'NN'), ('of', 'IN'), ('man', 'NN'), ('could', 'MD'), ('invent', 'VB'), ('.', '.')], [('We', 'PRP'), ('would', 'MD'), ('not', 'RB'), ('dare', 'VB'), ('to', 'TO'), ('conceive', 'VB'), ('the', 'DT'), ('things', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), ('really', 'RB'), ('mere', 'JJ'), ('commonplaces', 'NNS'), ('of', 'IN'), ('existence', 'NN'), ('.', '.')], [('If', 'IN'), ('we', 'PRP'), ('could', 'MD'), ('fly', 'VB'), ('out', 'IN'), ('of', 'IN'), ('that', 'DT'), ('window', 'JJ'), ('hand', 'NN'), ('in', 'IN'), ('hand', 'NN'), (',', ','), ('hover', 'NN'), ('over', 'IN'), ('this', 'DT'), ('great', 'JJ'), ('city', 'NN'), (',', ','), ('gently', 'RB'), ('remove', 'VB'), ('the', 'DT'), ('roofs', 'NN'), (',', ','), ('and', 'CC'), ('peep', 'NN'), ('in', 'IN'), ('at', 'IN'), ('the', 'DT'), ('queer', 'NN'), ('things', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), ('going', 'VBG'), ('on', 'IN'), (',', ','), ('the', 'DT'), ('strange', 'JJ'), ('coincidences', 'NNS'), (',', ','), ('the', 'DT'), ('plannings', 'NNS'), (',', ','), ('the', 'DT'), ('cross', 'NN'), ('-', ':'), ('purposes', 'NNS'), (',', ','), ('the', 'DT'), ('wonderful', 'JJ'), ('chains', 'NNS'), ('of', 'IN'), ('events', 'NNS'), (',', ','), ('working', 'VBG'), ('through', 'IN'), ('generations', 'NNS'), (',', ','), ('and', 'CC'), ('leading', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('most', 'RBS'), ('outrÃ', 'JJ'), ('©', 'NN'), ('results', 'NNS'), (',', ','), ('it', 'PRP'), ('would', 'MD'), ('make', 'VB'), ('all', 'DT'), ('fiction', 'NN'), ('with', 'IN'), ('its', 'PRP$'), ('conventionalities', 'NNS'), ('and', 'CC'), ('foreseen', 'JJ'), ('conclusions', 'NNS'), ('most', 'RBS'), ('stale', 'JJ'), ('and', 'CC'), ('unprofitable', 'JJ'), ('.\"', 'NN')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recommender.parse(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "459G9uIbsYNp",
        "outputId": "33b3ae4d-295b-4d48-980c-5cb8c71263e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Which', 'JJ'), ('stories', 'VBZ'), ('contain', 'NN'), ('door', 'NN'), (',', ','), ('child', 'NN'), (',', ','), ('woman', 'NN'), (',', ','), ('blood', 'NN'), (',', ','), ('money', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (recommender.listen(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQQkLx3WvHeu",
        "outputId": "8742b829-580d-4b0e-e9d5-f71385f1f402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EZiIffuWvnA"
      },
      "source": [
        "#To Do:\n",
        "\n",
        "\n",
        "1.   Try to fix the problem of low quality returned answer.\n",
        "2.   Try to return the paragraph(s) most relevant to the nouns specified in the question, together w/ the story's title.\n"
      ]
    }
  ]
}